{
  "hash": "327b65a4cab21769b88fa0575172ad4b",
  "result": {
    "markdown": "---\ntitle: \"MNE-BIDS-pipeline\"\nformat: revealjs\nexecute:\n  echo: true\n  eval: false\n---\n\n# MNE-BIDS-pipeline {.smaller}\n\n1.  Prepare your dataset\n2.  Create a configuration file\n3.  Run the pipeline\n\n## Prepare your dataset\n\nMNE-BIDS-Pipeline only works with BIDS-formatted raw data. To find out more about BIDS and how to convert your data to the BIDS format, please see the documentation of MNE-BIDS.\n\n![](https://mne.tools/mne-bids/assets/MNE-BIDS.png)\n\n## Create a configuration file {.smaller}\n\nAll parameters of the pipeline are controlled via a configuration file. Create a template configuration file by running the following command:\n\n`mne_bids_pipeline --create-config=/path/to/custom_config.py`\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\nstudy_name = \"ds000247\"\nbids_root = f\"~/mne_data/{study_name}\"\nderiv_root = f\"~/mne_data/derivatives/mne-bids-pipeline/{study_name}\"\n\nsubjects = [\"0002\"]\nsessions = [\"01\"]\ntask = \"rest\"\ntask_is_rest = True\n\ncrop_runs = (0, 100)  # to speed up computations\n\nch_types = [\"meg\"]\nspatial_filter = \"ssp\"\n\nl_freq = 1.0\nh_freq = 40.0\n\nrest_epochs_duration = 10\nrest_epochs_overlap = 0\n\nepochs_tmin = 0\nbaseline = None\n```\n:::\n\n\n-   [from example ds000247](https://mne.tools/mne-bids-pipeline/stable/examples/ds000247.html#configuration)\n\n## Run the pipeline {.smaller}\n\nTo run the full pipeline, simply call:\n\n`mne_bids_pipeline --config=/path/to/your/custom_config.py`\n\nTo run part of the pipeline, you can specify the stage you want to run:\n\n-   Run only the preprocessing steps:\n    -   `mne_bids_pipeline --config=/path/to/your/custom_config.py --steps=preprocessing`\n\n## Optional Parts of the pipeline {.smaller}\n\n-   Run only the sensor-level processing steps:\n    -   `mne_bids_pipeline --config=/path/to/your/custom_config.py --steps=sensor`\n-   Run only the source-level (inverse solution) processing steps:\n    -   `mne_bids_pipeline --config=/path/to/your/custom_config.py --steps=source`\n-   (Re-)run ICA:\n    -   `mne_bids_pipeline --config=/path/to/your/custom_config.py --steps=preprocessing/ica`\n-   You can also run multiple steps with one command by separating different steps by a comma. For example, to run preprocessing and sensor-level processing steps using a single command, do:\n    -   `mne_bids_pipeline --config=/path/to/your/custom_config.py --steps=preprocessing,sensor`\n\n# Running the pipeline on HiPerGator {.smaller}\n\n## Log on to HiPerGator terminal/shell {.smaller}\n\nConnect to UF Research Computing HPG3 Cluster from terminal on macOS\n\nType the following command in the terminal:\n\n`ssh` *urgatoruser*`@hpg.rc.ufl.edu`\n\nThen, follow the prompts to enter your password and Duo two-factor authentication.\n\n**Your password will not be displayed as you type it.** Then, your terminal should look like this:\n\n``` bash\nssh urgatoruser@hpg.rc.ufl.edu\nPassword: xxxxxxxx\nDuo two-factor login for urgatoruser@ufl.edu\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-1809\n 2. Phone call to XXX-XXX-1809\n\nPasscode or option (1-2): 1\n```\n\nWorks through the UF Single-sign on similar to for Canvas and gatormail.\n\n## Start a compute node on HiPerGator {.smaller}\n\n-   single node 2 CPU core job with 2gb of RAM for 90 minutes can be started with the following command\n\n``` bash\nsrun --account=psb4934 --qos=psb4934 --ntasks=1 --cpus-per-task=2 --mem=2gb -t 90 --pty bash -i\n```\n\n`[mygatoruser@login1 ~]$ srun --pty -p hpg2-compute -n 1 -N 1 -t 90 --mem=2gb /bin/bash`\n\n-   single node 4 CPU core job with 28gb of RAM for 120 minutes can be started with the following command\n\n``` bash\nsrun --account=psb4934 --qos=psb4934 --ntasks=1 --cpus-per-task=4 --mem=28gb -t 60 --pty bash -i\n```\n\n## Check that your job is running\n\nNow you are on the compute node. You can check the hostname and the node list with the following commands:\n\n``` bash\necho \"Hello from $(hostname)\"\n```\n\n`Hello from c0711a-s6.ufhpc`\n\n``` bash\necho $SLURM_JOB_NODELIST\n```\n\n`c0711a-s6`\n\n## Activate: Conda-env for pipeline {.smaller}\n\nNavigate to group storage on Blue drive then activate conda environment to have access to pipeline\n\n``` bash\ncd /blue/akeil/share/mears/misophonia/miso-project-pipeline\n\nmodule load conda\nconda activate mne_v1_6\n```\n\n# `Deeper Understanding`: behind functions of the MNE-BIDS-pipeline \n\n## Core scripts of pipeline {.smaller}\n\n::: columns\n::: {.column width=\"25%\"}\n#### Essential Sequence\n\n-   `__init__.py`\n\n-   `_config.py`\n\n-   `_main.py`\n\n-   `_run.py`\n:::\n\n::: {.column width=\"35%\"}\n#### Supporting Scripts\n\n-   `_config_import.py`\n-   `_config_template.py`\n-   `_config_utils.py`\n-   `_logging.py`\n-   `_parallel.py`\n-   `_io.py`\n:::\n\n::: {.column width=\"25%\"}\n#### Specific Procedures\n\n-   `_decoding.py`\n\n-   `_reject.py`\n\n-   `_viz.py`\n\n-   `_report.py`\n\n-   `_download.py`\n\n-   `_import_data.py`\n:::\n:::\n\n## Stages / Steps\n\n-   init\n\n-   fresurfer\n\n-   preprocessing\n\n-   sensor\n\n-   source\n\n# Main Script\n\n## Imports and Definitions {.smaller}\n\nThe script starts by importing necessary modules and defining some functions. This includes modules for argument parsing (`argparse`), file path handling (`pathlib`), logging, parallel processing, and specific functions from other modules within the same package (prefixed with `_`).\n\n::: {style=\"display: flex; position: relative;\"}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport argparse\nimport pathlib\nfrom textwrap import dedent\nimport time\nfrom typing import List\nfrom types import ModuleType, SimpleNamespace\n\nimport numpy as np\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom ._config_utils import _get_step_modules\nfrom ._config_import import _import_config\nfrom ._config_template import create_template_config\nfrom ._logging import logger, gen_log_kwargs\nfrom ._parallel import get_parallel_backend\nfrom ._run import _short_step_path\n```\n:::\n\n\n:::\n\n\\\n`argparse` - [Good resource for understanding argparse](https://realpython.com/command-line-interfaces-python-argparse/#creating-command-line-interfaces-with-pythons-argparse)\n\n\\\n\n#### `text manipulation` of\n\n#### command line input =\n\n::: {.fragment .fade-in-then-semi-out}\n### üí´*Pipeline*üßö‚Äç‚ôÄÔ∏è*magic*‚ú®\n:::\n\n## Argument Parsing {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n\nThe script uses the `argparse` module to define command-line arguments that control the behavior of the pipeline.\n\n-   `config`: Path to a configuration file specifying pipeline settings.\n-   `create-config`: Creates a template configuration file.\n-   `steps`: Defines specific processing steps or groups of steps to run.\n-   `root-dir`, `deriv_root`, `subject`, `session`, `task`, `run`: Specify paths and identifiers for the data to process.\n-   `n_jobs`: Number of parallel processes to execute.\n-   `interactive`, `debug`, `no-cache`: Flags for interactive mode, debugging, and disabling caching.\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--version\", action=\"version\", version=f\"%(prog)s {__version__}\"\n    )\n    parser.add_argument(\"config\", nargs=\"?\", default=None)\n    parser.add_argument(\n        \"--config\",\n        dest=\"config_switch\",\n        default=None,\n        metavar=\"FILE\",\n        help=\"The path of the pipeline configuration file to use.\",\n    )\n    parser.add_argument(\n        \"--create-config\",\n        dest=\"create_config\",\n        default=None,\n        metavar=\"FILE\",\n        help=\"Create a template configuration file with the specified name. \"\n        \"If specified, all other parameters will be ignored.\",\n    ),\n    parser.add_argument(\n        \"--steps\",\n        dest=\"steps\",\n        default=\"all\",\n        help=dedent(\n            \"\"\"\\\n        The processing steps to run.\n        Can either be one of the processing groups 'preprocessing', sensor',\n        'source', 'report',  or 'all',  or the name of a processing group plus\n        the desired step sans the step number and\n        filename extension, separated by a '/'. For example, to run ICA, you\n        would pass 'sensor/run_ica`. If unspecified, will run all processing\n        steps. Can also be a tuple of steps.\"\"\"\n        ),\n    )\n    parser.add_argument(\n        \"--root-dir\",\n        dest=\"root_dir\",\n        default=None,\n        help=\"BIDS root directory of the data to process.\",\n    )\n    parser.add_argument(\n        \"--deriv_root\",\n        dest=\"deriv_root\",\n        default=None,\n        help=dedent(\n            \"\"\"\\\n        The root of the derivatives directory\n        in which the pipeline will store the processing results.\n        If unspecified, this will be derivatives/mne-bids-pipeline\n        inside the BIDS root.\"\"\"\n        ),\n    ),\n    parser.add_argument(\n        \"--subject\", dest=\"subject\", default=None, help=\"The subject to process.\"\n    )\n    parser.add_argument(\n        \"--session\", dest=\"session\", default=None, help=\"The session to process.\"\n    )\n    parser.add_argument(\n        \"--task\", dest=\"task\", default=None, help=\"The task to process.\"\n    )\n    parser.add_argument(\"--run\", dest=\"run\", default=None, help=\"The run to process.\")\n    parser.add_argument(\n        \"--n_jobs\",\n        dest=\"n_jobs\",\n        type=int,\n        default=None,\n        help=\"The number of parallel processes to execute.\",\n    )\n    parser.add_argument(\n        \"--interactive\",\n        dest=\"interactive\",\n        action=\"store_true\",\n        help=\"Enable interactive mode.\",\n    )\n    parser.add_argument(\n        \"--debug\", dest=\"debug\", action=\"store_true\", help=\"Enable debugging on error.\"\n    )\n    parser.add_argument(\n        \"--no-cache\",\n        dest=\"no_cache\",\n        action=\"store_true\",\n        help=\"Disable caching of intermediate results.\",\n    )\n    options = parser.parse_args()\n\n    if options.create_config is not None:\n        target_path = pathlib.Path(options.create_config)\n        create_template_config(target_path=target_path, overwrite=False)\n        return\n\n```\n:::\n\n\n:::\n:::\n\n## Argument Parsing{.smaller}\n\n| Argument     | Dest     | Help                                                                                                      |\n|---------------|------------|----------------------------------------------------------------------------------------------------------|\n| `--version`       | (auto)             | version  |         | (Version information)                                                                                     |\n| `config`          | (auto)                | (Positional argument)                                                                                     |\n| `--config`        | config_switch         | The path of the pipeline configuration file to use.                                                       |\n| `--create-config` | create_config         | Create a template configuration file with the specified name. If specified, all other parameters ignored. |\n| `--steps`         | steps                      | The processing steps to run. Various options detailed in help.                                            |\n| `--root-dir`      | root_dir                  | BIDS root directory of the data to process.                                                               |\n| `--deriv_root`    | deriv_root            | Root of the derivatives directory to store the processing results.                                        |\n| `--subject`       | subject                  | The subject to process.                                                                                   |\n| `--session`       | session                   | The session to process.                                                                                   |\n| `--task`          | task                    | The task to process.                                                                                      |\n\n## Argument Parsing{.smaller}\n\n| Argument          | Dest           | Action   | Help                                                                                                      |\n|-------------------|----------------|----------|----------------------------------------------------------------------------------------------------------|\n| `--run`           | run              |             | The run to process.                                                                                       |\n| `--n_jobs`        | n_jobs        |            | The number of parallel processes to execute.                                                              |\n| `--interactive`   | interactive      | store_true   | Enable interactive mode.                                                                                  |\n| `--debug`         | debug           | store_true    | Enable debugging on error.                                                                                |\n| `--no-cache`      | no_cache        | store_true       | Disable caching of intermediate results.                                                                  |\n\n\n- The `dest` column represents the name of the attribute under which the argument's value will be stored. If not specified, default is long option name (w/out the initial `--`).\n- The `default` column shows the default value for the argument.\n- The `type` column lists the data type that argparse will try to convert the argument value to. If not specified, it defaults to `str`.\n- The `action` column specifies action to be taken when argument is encountered.\n- The `help` column contains a brief description of what the argument does. In this table, detailed descriptions are summarized or indicated in parentheses.\n- The `metavar` column indicates how argument is provided to help message.\n\n## Argument Validation {.smaller}\n\nThe script checks if the provided arguments are valid, particularly focusing on the configuration file.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n     options = parser.parse_args()\n\n    if options.create_config is not None:\n        target_path = pathlib.Path(options.create_config)\n        create_template_config(target_path=target_path, overwrite=False)\n        return\n\n    config = options.config\n    config_switch = options.config_switch\n    bad = False\n    if config is None:\n        if config_switch is None:\n            bad = \"neither was provided\"\n        else:\n            config = config_switch\n    elif config_switch is not None:\n        bad = \"both were provided\"\n    if bad:\n        parser.error(\n            \"‚ùå You must specify a configuration file either as a single \"\n            f\"argument or with --config, but {bad}.\"\n        )\n    steps = options.steps\n    root_dir = options.root_dir\n    deriv_root = options.deriv_root\n    subject, session = options.subject, options.session\n    task, run = options.task, options.run\n    n_jobs = options.n_jobs\n    interactive, debug = options.interactive, options.debug\n    cache = not options.no_cache\n\n    if isinstance(steps, str) and \",\" in steps:\n        # Work around limitation in Fire: --steps=foo,bar/baz won't produce a\n        # tuple ('foo', 'bar/baz'), but a string 'foo,bar/baz'.\n        steps = tuple(steps.split(\",\"))\n    elif isinstance(steps, str):\n        steps = (steps,)\n\n    on_error = \"debug\" if debug else None\n    cache = \"1\" if cache else \"0\"\n\n    processing_stages = []\n    processing_steps = []\n    for steps_ in steps:\n        if \"/\" in steps_:\n            stage, step = steps_.split(\"/\")\n            processing_stages.append(stage)\n            processing_steps.append(step)\n        else:\n            # User specified \"sensor\", \"preprocessing\" or similar, but without\n            # any further grouping.\n            processing_stages.append(steps_)\n            processing_steps.append(None)\n```\n:::\n\n\n## Processing Steps Identification {.smaller}\n\nBased on the `steps` argument, the script identifies which processing steps or stages are to be executed. It does this by parsing the `steps` argument and mapping them to corresponding modules.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n    step_modules: List[ModuleType] = []\n    STEP_MODULES = _get_step_modules()\n    for stage, step in zip(processing_stages, processing_steps):\n        if stage not in STEP_MODULES.keys():\n            raise ValueError(\n                f\"Invalid step requested: '{stage}'. \"\n                f\"It should be one of {list(STEP_MODULES.keys())}.\"\n            )\n\n        if step is None:\n            # User specified `sensors`, `source`, or similar\n            step_modules.extend(STEP_MODULES[stage])\n        else:\n            # User specified 'stage/step'\n            for step_module in STEP_MODULES[stage]:\n                step_name = pathlib.Path(step_module.__file__).name\n                if step in step_name:\n                    step_modules.append(step_module)\n                    break\n            else:\n                # We've iterated over all steps, but none matched!\n                raise ValueError(f\"Invalid steps requested: {stage}/{step}\")\n\n    if processing_stages[0] != \"all\":\n        # Always run the directory initialization steps, but skip for 'all',\n        # because it already includes them ‚Äì and we want to avoid running\n        # them twice.\n        step_modules = [*STEP_MODULES[\"init\"], *step_modules]\n\n```\n:::\n\n\n## Configuration Loading and Overrides {.smaller}\n\nThe script loads the pipeline configuration from the specified file and applies any overrides from the command-line arguments.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n     options = parser.parse_args()\n\n    if options.create_config is not None:\n        target_path = pathlib.Path(options.create_config)\n        create_template_config(target_path=target_path, overwrite=False)\n        return\n\n    config = options.config\n    config_switch = options.config_switch\n    bad = False\n    if config is None:\n        if config_switch is None:\n            bad = \"neither was provided\"\n        else:\n            config = config_switch\n    elif config_switch is not None:\n        bad = \"both were provided\"\n    if bad:\n        parser.error(\n            \"‚ùå You must specify a configuration file either as a single \"\n            f\"argument or with --config, but {bad}.\"\n        )\n    steps = options.steps\n    root_dir = options.root_dir\n    deriv_root = options.deriv_root\n    subject, session = options.subject, options.session\n    task, run = options.task, options.run\n    n_jobs = options.n_jobs\n    interactive, debug = options.interactive, options.debug\n    cache = not options.no_cache\n\n    if isinstance(steps, str) and \",\" in steps:\n        # Work around limitation in Fire: --steps=foo,bar/baz won't produce a\n        # tuple ('foo', 'bar/baz'), but a string 'foo,bar/baz'.\n        steps = tuple(steps.split(\",\"))\n    elif isinstance(steps, str):\n        steps = (steps,)\n\n    on_error = \"debug\" if debug else None\n    cache = \"1\" if cache else \"0\"\n\n    processing_stages = []\n    processing_steps = []\n    for steps_ in steps:\n        if \"/\" in steps_:\n            stage, step = steps_.split(\"/\")\n            processing_stages.append(stage)\n            processing_steps.append(step)\n        else:\n            # User specified \"sensor\", \"preprocessing\" or similar, but without\n            # any further grouping.\n            processing_stages.append(steps_)\n            processing_steps.append(None)\n\n    config_path = pathlib.Path(config).expanduser().resolve(strict=True)\n    overrides = SimpleNamespace()\n    if root_dir:\n        overrides.bids_root = pathlib.Path(root_dir).expanduser().resolve(strict=True)\n    if deriv_root:\n        overrides.deriv_root = (\n            pathlib.Path(deriv_root).expanduser().resolve(strict=False)\n        )\n    if subject:\n        overrides.subjects = [subject]\n    if session:\n        overrides.sessions = [session]\n    if task:\n        overrides.task = task\n    if run:\n        overrides.runs = run\n    if interactive:\n        overrides.interactive = interactive\n    if n_jobs:\n        overrides.n_jobs = int(n_jobs)\n    if on_error:\n        overrides.on_error = on_error\n    if not cache:\n        overrides.memory_location = False\n```\n:::\n\n\n## Pipeline Execution {.smaller}\n\n-   The script iterates over the identified processing steps.\n-   For each step, it logs the start, executes the main function of the corresponding module (which is where the actual processing happens), and then logs the time taken for the step.\n-   This execution can be parallelized based on the `n_jobs` argument.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n    # Initialize dask now\n    with get_parallel_backend(config_imported.exec_params):\n        pass\n    del __mne_bids_pipeline_step__\n    logger.end()\n\n\n    for step_module in step_modules:\n        start = time.time()\n        step = _short_step_path(pathlib.Path(step_module.__file__))\n        logger.title(title=f\"{step}\")\n        step_module.main(config=config_imported)\n        elapsed = time.time() - start\n        hours, remainder = divmod(elapsed, 3600)\n        hours = int(hours)\n        minutes, seconds = divmod(remainder, 60)\n        minutes = int(minutes)\n        seconds = int(np.ceil(seconds))  # always take full seconds\n        elapsed = f\"{seconds}s\"\n        if minutes:\n            elapsed = f\"{minutes}m {elapsed}\"\n        if hours:\n            elapsed = f\"{hours}h {elapsed}\"\n        logger.end(f\"done ({elapsed})\")\n```\n:::\n\n\n## Logging and Error Handling {.smaller}\n\nThroughout its execution, the script logs various messages, including errors and execution time for each step. The `--debug` option enables additional debugging information on error.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n    interactive, debug = options.interactive, options.debug\n    cache = not options.no_cache\n\n    if isinstance(steps, str) and \",\" in steps:\n        # Work around limitation in Fire: --steps=foo,bar/baz won't produce a\n        # tuple ('foo', 'bar/baz'), but a string 'foo,bar/baz'.\n        steps = tuple(steps.split(\",\"))\n    elif isinstance(steps, str):\n        steps = (steps,)\n\n    on_error = \"debug\" if debug else None\n    cache = \"1\" if cache else \"0\"\n```\n:::\n\n\n## Pipeline Configuration and Modular Design {.smaller}\n\nThe script is designed to be modular, where each step in the pipeline is encapsulated in a separate module. The pipeline's behavior is controlled by a configuration file, allowing for flexible and customizable data processing.\n\n\n\n## Interactive and Cache Options {.smaller}\n\n\nThe script supports interactive mode and can disable caching of intermediate results, providing flexibility for different use cases.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n    interactive, debug = options.interactive, options.debug\n    cache = not options.no_cache\n\n    if isinstance(steps, str) and \",\" in steps:\n        # Work around limitation in Fire: --steps=foo,bar/baz won't produce a\n        # tuple ('foo', 'bar/baz'), but a string 'foo,bar/baz'.\n        steps = tuple(steps.split(\",\"))\n    elif isinstance(steps, str):\n        steps = (steps,)\n\n    on_error = \"debug\" if debug else None\n    cache = \"1\" if cache else \"0\"\n```\n:::\n\n\n## Overriding Configuration Options {.smaller}\n\nThe script allows for overriding configuration options from the command line, providing flexibility for different use cases.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n    config_path = pathlib.Path(config).expanduser().resolve(strict=True)\n    overrides = SimpleNamespace()\n    if root_dir:\n        overrides.bids_root = pathlib.Path(root_dir).expanduser().resolve(strict=True)\n    if deriv_root:\n        overrides.deriv_root = (\n            pathlib.Path(deriv_root).expanduser().resolve(strict=False)\n        )\n    if subject:\n        overrides.subjects = [subject]\n    if session:\n        overrides.sessions = [session]\n    if task:\n        overrides.task = task\n    if run:\n        overrides.runs = run\n    if interactive:\n        overrides.interactive = interactive\n    if n_jobs:\n        overrides.n_jobs = int(n_jobs)\n    if on_error:\n        overrides.on_error = on_error\n    if not cache:\n        overrides.memory_location = False\n```\n:::\n\n\n## Summary\n\nOverall, the script is a command-line interface for a data processing pipeline, where the specific processing steps are modularized and controlled via a configuration file. The use of command-line arguments allows for flexible execution of different parts of the pipeline.\n\n# Configuration\n\n## Configuration File {.smaller}\n\nThe configuration file is a Python file that contains a dictionary with the configuration parameters. The configuration file is used to control the behavior of the pipeline. It is passed to the pipeline script via the `--config` argument.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\n\n\nstudy_name = \"ds000247\"\nbids_root = f\"~/mne_data/{study_name}\"\nderiv_root = f\"~/mne_data/derivatives/mne-bids-pipeline/{study_name}\"\n\nsubjects = [\"0002\"]\nsessions = [\"01\"]\ntask = \"rest\"\ntask_is_rest = True\n\ncrop_runs = (0, 100)  # to speed up computations\n\nch_types = [\"meg\"]\nspatial_filter = \"ssp\"\n\nl_freq = 1.0\nh_freq = 40.0\n\nrest_epochs_duration = 10\nrest_epochs_overlap = 0\n\nepochs_tmin = 0\nbaseline = None\n```\n:::\n\n\n-   [from example ds000247](https://mne.tools/mne-bids-pipeline/stable/examples/ds000247.html#configuration)\n\n## Configuration Parameters {.smaller}\n\n## \n\n",
    "supporting": [
      "intro_files"
    ],
    "filters": [],
    "includes": {}
  }
}